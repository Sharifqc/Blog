{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will be varying the architecture of the neural network, similar as to what was done in the [scikit-learn version](http://ericstrong.org/predicting-abalone-rings-part-3-multilayer-perceptron/) of this analysis. \n",
    "\n",
    "First, the data is loaded and preprocessed as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing from Part 1\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "abalone_df = pd.read_csv('abalone.csv',names=['Sex','Length','Diameter','Height',\n",
    "    'Whole Weight','Shucked Weight', 'Viscera Weight','Shell Weight', 'Rings'])\n",
    "abalone_df['Male'] = (abalone_df['Sex']=='M').astype(int)\n",
    "abalone_df['Female'] = (abalone_df['Sex']=='F').astype(int)\n",
    "abalone_df['Infant'] = (abalone_df['Sex']=='I').astype(int)\n",
    "abalone_df = abalone_df[abalone_df['Height']>0]\n",
    "train, test = train_test_split(abalone_df, train_size=0.7)\n",
    "x_train = train.drop(['Rings','Sex'], axis=1).values\n",
    "y_train = pd.DataFrame(train['Rings']).values\n",
    "x_test = test.drop(['Rings','Sex'], axis=1).values\n",
    "y_test = pd.DataFrame(test['Rings']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To automatically construct a list of models to test, I used a listcomp in the following code. Each of these models requires exactly two hidden layers. The idea is that I will test a range of values from 5 to 20 for each hidden layer, incrementing by 5 each time. I also extended the list with three additional models that I thought would be interesting to test as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constructing a list of models to test\n",
    "hlayers = [[x,y] for x in range(5,21,5) for y in range(5,21,5)]\n",
    "hlayers.extend([[1,10],[10,1],[2,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will iterate over each of the above models, trying three different activation functions: 'tanh', 'relu', and 'sigmoid'. The same model-building procedure as in Part 1 of this blog series will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1253/1253 [==============================] - 0s     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1253/1253 [==============================] - 0s      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      " 928/1253 [=====================>........] - ETA: 0s \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bTotal elapsed seconds: 674.114391\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the list of models, trying 3 different activation functions\n",
    "begin = datetime.datetime.now()\n",
    "results_dict = {}\n",
    "for act in ['tanh', 'relu', 'sigmoid']:\n",
    "    for layers in hlayers:\n",
    "        abalone_model = Sequential([\n",
    "            Dense(layers[0], input_dim=10, activation=act),\n",
    "            Dense(layers[1], activation=act),\n",
    "            Dense(1)])\n",
    "        abalone_model.compile(optimizer='rmsprop',loss='mse',metrics=[\"mean_absolute_error\"])\n",
    "        results = abalone_model.fit(x_train, y_train, nb_epoch=50, verbose=0)\n",
    "        score = abalone_model.evaluate(x_test, y_test)\n",
    "        result_string = \"[{},{}] {}\".format(layers[0], layers[1], act)\n",
    "        results_dict[result_string] = score[1]\n",
    "# Save the results in a DataFrame\n",
    "results_df = pd.DataFrame.from_dict(results_dict, orient=\"index\")\n",
    "results_df.rename(columns={0 : \"MAE\"}, inplace=True)\n",
    "seconds = (datetime.datetime.now() - begin).total_seconds()\n",
    "sec_string = \"Total elapsed seconds: {}\".format(seconds)\n",
    "print(sec_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the total elapsed time was about 10 minutes. I'm running this code on a fairly old Linux computer. I have no doubt that it would run faster with better hardware. However, in the following post, I'll investigate using the GPU to run the neural networks, rather than the CPU, which should provide very significant speed increases.\n",
    "\n",
    "Finally, the following code will present the results in tabular form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[20,20] relu</th>\n",
       "      <td>1.521228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10,15] relu</th>\n",
       "      <td>1.556953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[15,5] relu</th>\n",
       "      <td>1.560309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[15,10] relu</th>\n",
       "      <td>1.563199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[20,10] relu</th>\n",
       "      <td>1.574703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[5,20] relu</th>\n",
       "      <td>1.574799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[15,15] tanh</th>\n",
       "      <td>1.578155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10,20] tanh</th>\n",
       "      <td>1.582573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[20,10] tanh</th>\n",
       "      <td>1.583966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10,5] relu</th>\n",
       "      <td>1.591987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[20,5] tanh</th>\n",
       "      <td>1.592549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10,20] relu</th>\n",
       "      <td>1.596836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[15,20] relu</th>\n",
       "      <td>1.600640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[15,15] relu</th>\n",
       "      <td>1.600932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[5,20] tanh</th>\n",
       "      <td>1.604035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[5,15] relu</th>\n",
       "      <td>1.608090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[20,15] relu</th>\n",
       "      <td>1.612473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10,5] tanh</th>\n",
       "      <td>1.614221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[5,10] tanh</th>\n",
       "      <td>1.618094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10,10] relu</th>\n",
       "      <td>1.619198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[5,10] relu</th>\n",
       "      <td>1.619235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[20,5] relu</th>\n",
       "      <td>1.619708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[15,10] tanh</th>\n",
       "      <td>1.626951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10,10] tanh</th>\n",
       "      <td>1.628885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[20,15] tanh</th>\n",
       "      <td>1.634970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[5,5] tanh</th>\n",
       "      <td>1.638008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10,15] tanh</th>\n",
       "      <td>1.646904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[15,5] tanh</th>\n",
       "      <td>1.655188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[5,5] relu</th>\n",
       "      <td>1.670591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[20,20] tanh</th>\n",
       "      <td>1.678611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[5,15] tanh</th>\n",
       "      <td>1.692873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[15,20] tanh</th>\n",
       "      <td>1.719730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[1,10] tanh</th>\n",
       "      <td>1.743584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10,1] relu</th>\n",
       "      <td>1.761204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[15,20] sigmoid</th>\n",
       "      <td>1.788707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[2,2] relu</th>\n",
       "      <td>1.791604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[20,10] sigmoid</th>\n",
       "      <td>1.795285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[15,15] sigmoid</th>\n",
       "      <td>1.805593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[20,15] sigmoid</th>\n",
       "      <td>1.808499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10,20] sigmoid</th>\n",
       "      <td>1.817504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[1,10] relu</th>\n",
       "      <td>1.824509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[20,20] sigmoid</th>\n",
       "      <td>1.826380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10,10] sigmoid</th>\n",
       "      <td>1.841199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[20,5] sigmoid</th>\n",
       "      <td>1.843298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[5,15] sigmoid</th>\n",
       "      <td>1.844008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[15,10] sigmoid</th>\n",
       "      <td>1.850834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[5,10] sigmoid</th>\n",
       "      <td>1.857714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[15,5] sigmoid</th>\n",
       "      <td>1.862459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10,15] sigmoid</th>\n",
       "      <td>1.868373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10,5] sigmoid</th>\n",
       "      <td>1.876700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[5,5] sigmoid</th>\n",
       "      <td>1.881083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[5,20] sigmoid</th>\n",
       "      <td>1.893886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[1,10] sigmoid</th>\n",
       "      <td>1.894800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[2,2] tanh</th>\n",
       "      <td>1.915620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10,1] tanh</th>\n",
       "      <td>1.971139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10,1] sigmoid</th>\n",
       "      <td>2.333878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[2,2] sigmoid</th>\n",
       "      <td>2.345666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      MAE\n",
       "[20,20] relu     1.521228\n",
       "[10,15] relu     1.556953\n",
       "[15,5] relu      1.560309\n",
       "[15,10] relu     1.563199\n",
       "[20,10] relu     1.574703\n",
       "[5,20] relu      1.574799\n",
       "[15,15] tanh     1.578155\n",
       "[10,20] tanh     1.582573\n",
       "[20,10] tanh     1.583966\n",
       "[10,5] relu      1.591987\n",
       "[20,5] tanh      1.592549\n",
       "[10,20] relu     1.596836\n",
       "[15,20] relu     1.600640\n",
       "[15,15] relu     1.600932\n",
       "[5,20] tanh      1.604035\n",
       "[5,15] relu      1.608090\n",
       "[20,15] relu     1.612473\n",
       "[10,5] tanh      1.614221\n",
       "[5,10] tanh      1.618094\n",
       "[10,10] relu     1.619198\n",
       "[5,10] relu      1.619235\n",
       "[20,5] relu      1.619708\n",
       "[15,10] tanh     1.626951\n",
       "[10,10] tanh     1.628885\n",
       "[20,15] tanh     1.634970\n",
       "[5,5] tanh       1.638008\n",
       "[10,15] tanh     1.646904\n",
       "[15,5] tanh      1.655188\n",
       "[5,5] relu       1.670591\n",
       "[20,20] tanh     1.678611\n",
       "[5,15] tanh      1.692873\n",
       "[15,20] tanh     1.719730\n",
       "[1,10] tanh      1.743584\n",
       "[10,1] relu      1.761204\n",
       "[15,20] sigmoid  1.788707\n",
       "[2,2] relu       1.791604\n",
       "[20,10] sigmoid  1.795285\n",
       "[15,15] sigmoid  1.805593\n",
       "[20,15] sigmoid  1.808499\n",
       "[10,20] sigmoid  1.817504\n",
       "[1,10] relu      1.824509\n",
       "[20,20] sigmoid  1.826380\n",
       "[10,10] sigmoid  1.841199\n",
       "[20,5] sigmoid   1.843298\n",
       "[5,15] sigmoid   1.844008\n",
       "[15,10] sigmoid  1.850834\n",
       "[5,10] sigmoid   1.857714\n",
       "[15,5] sigmoid   1.862459\n",
       "[10,15] sigmoid  1.868373\n",
       "[10,5] sigmoid   1.876700\n",
       "[5,5] sigmoid    1.881083\n",
       "[5,20] sigmoid   1.893886\n",
       "[1,10] sigmoid   1.894800\n",
       "[2,2] tanh       1.915620\n",
       "[10,1] tanh      1.971139\n",
       "[10,1] sigmoid   2.333878\n",
       "[2,2] sigmoid    2.345666"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the results matrix\n",
    "results_df.sort_values('MAE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best results came from a neural network with 20 nodes in each hidden layer and a \"relu\" activation function. The top 20 networks or so had fairly similar results- all were likely within the testing accuracy of the network. Given these results, I would likely choose the \"[10,5] relu\" network, given its simplicity. Comparing these results to the previous blog post using the scikit-learn neural network module, I am surprised that the \"relu\" activation function performed so well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
